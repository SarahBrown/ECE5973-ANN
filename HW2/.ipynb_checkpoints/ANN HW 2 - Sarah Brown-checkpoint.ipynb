{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-circulation",
   "metadata": {},
   "source": [
    "# ECE 5973 - Artificial Neural Networks\n",
    "## Homework 2\n",
    "### Sarah Brown\n",
    "\n",
    "\n",
    "# Question 1\n",
    "(10 points) Follow the derivation in the tutorial, show that the dual optimization problem of above is indeed shown as (10) there by filling in any missing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-cinema",
   "metadata": {},
   "source": [
    "With the type of training data we are given, we want to find a function f(x) that has a epsilon small deviation from the obtained targets for all of the training data. This means that we will not accept any errors that are larger than epsilon.\n",
    "\n",
    "Start this deviation with linear function f as shown below. With <.,.> representing a dot product.\n",
    "\n",
    "$$f(x)=<w,x> + b \\\\ with w \\in X, b \\in R$$\n",
    "\n",
    "So we want f(x) to be \"flat.\" Flat here means that we want a small w. It is possible to get a small w by minimizing the norm\n",
    "\n",
    "$$||w||^2 = <w,w>$$\n",
    "\n",
    "So we want something that minimizes:\n",
    "$$\\frac{1}{2} ||w|^2 \\\\\n",
    "with: \\ y_i - <w,x_i> - b \\leq \\epsilon \\\\\n",
    "and \\ with: \\ <w,x_i> + b - y_i \\leq \\epsilon$$ \n",
    "\n",
    "The above comes with an assumption that it is possible to approximate the targets y with the training data x with a function with a preffered precision. This might not be possible in all cases. \n",
    "\n",
    "Can also add slack variables to adjust for some of the weird constraints of the optimization. This gives:\n",
    "\n",
    "$$\\frac{1}{2} ||w|^2 + C \\Sigma_{i=1} ^l (\\xi_i + \\xi_i^*) \\\\\n",
    "with: \\ y_i - <w,x_i> - b \\leq \\epsilon \\\\\n",
    "and \\ with: \\ <w,x_i> + b - y_i \\leq \\epsilon \\\\\n",
    "and \\ with: \\xi_i , \\xi_i^* \\geq 0$$ \n",
    "\n",
    "The new constant C adjusts flatness of f and the number of deviations larger than epsilon. This gives us an epsilon-insensitive loss function:\n",
    "$$|\\xi_{\\epsilon} := \\begin{cases}\n",
    "0 & \\text{if $|\\xi| \\leq \\epsilon$} \\\\\n",
    "|\\xi|-\\epsilon & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "The derivation continues by constructing a Lagrange function and showing that the function has saddle points. This Lagrange function is defined as follows.\n",
    "\n",
    "$$ L := \\frac{1}{2} ||w||^2 + C \\Sigma_{i=1} ^l (\\xi_i + \\xi_i^*) - \\Sigma_{i=1} ^l (\\eta_i \\xi_i + \\eta_i^*\\xi_i^*) - \\Sigma_{i=1} ^l \\alpha_i (\\epsilon + \\xi_i - y_i + <w,x_i> +b) - \\Sigma_{i=1} ^l \\alpha_i^* (\\epsilon + \\xi_i^* + y_i - <w,x_i> - b)$$\n",
    "\n",
    "With $\\eta_i, \\eta_i^*, \\alpha_i, \\alpha_i^*$ as Lagrange multiplers.\n",
    "\n",
    "These variables have to be positive. In addition, since we want it to have a saddle point this means that the partial derivatives of L go away. This seems to be due to the behavior of Lagrange functions in general. \n",
    "\n",
    "The partial derivatives of with respect to the primal variables are:\n",
    "\n",
    "$$\\delta_b L = \\delta_b(frac{1}{2} ||w||^2 + C \\Sigma_{i=1} ^l (\\xi_i + \\xi_i^*) - \\Sigma_{i=1} ^l (\\eta_i \\xi_i + \\eta_i^*\\xi_i^*) - \\Sigma_{i=1} ^l \\alpha_i (\\epsilon + \\xi_i - y_i + <w,x_i> +b) - \\Sigma_{i=1} ^l \\alpha_i^* (\\epsilon + \\xi_i^* + y_i - <w,x_i> - b)) \\\\\n",
    "= 0 + C*0 - 0 - \\Sigma_{i=1}^l \\alpha_i(0+0-0+0+1) - \\Sigma_{i=1}^l (\\alpha_i^* (0+0+0-0-b) \\\\ \n",
    "\\rightarrow \\delta_bL = \\Sigma_{i=1}^l(\\alpha_i^* -\\alpha_i)$$\n",
    "\n",
    "$$\\delta_w L = \\delta_w(frac{1}{2} ||w||^2 + C \\Sigma_{i=1} ^l (\\xi_i + \\xi_i^*) - \\Sigma_{i=1} ^l (\\eta_i \\xi_i + \\eta_i^*\\xi_i^*) - \\Sigma_{i=1} ^l \\alpha_i (\\epsilon + \\xi_i - y_i + <w,x_i> +b) - \\Sigma_{i=1} ^l \\alpha_i^* (\\epsilon + \\xi_i^* + y_i - <w,x_i> - b)) \\\\\n",
    "= w - \\Sigma_{i=1}^l \\alpha_i(x_i) - \\Sigma_{i=1}^l (\\alpha_i^* (-x_i) \\\\ \n",
    "\\rightarrow \\delta_wL = w - \\Sigma_{i=1}^l x_i(\\alpha_i^ -\\alpha_i^*)$$\n",
    "\n",
    "$$\\delta_{\\xi_i^{(*)}} L = \\delta_{\\xi_i^{(*)}}(frac{1}{2} ||w||^2 + C \\Sigma_{i=1} ^l (\\xi_i + \\xi_i^*) - \\Sigma_{i=1} ^l (\\eta_i \\xi_i + \\eta_i^*\\xi_i^*) - \\Sigma_{i=1} ^l \\alpha_i (\\epsilon + \\xi_i - y_i + <w,x_i> +b) - \\Sigma_{i=1} ^l \\alpha_i^* (\\epsilon + \\xi_i^* + y_i - <w,x_i> - b)) \\\\\n",
    "\\delta_{\\xi_i^{(*)}}L = \\delta_{\\xi_i^*}L = C - \\alpha_i^{(*)} - \\eta_i^{(*)}$$\n",
    "\n",
    "Where $\\xi_i^{(*)}$, $\\alpha_i^{(*)}$ and $\\eta_i^{(*)}$ refer to the pair of variables with *'s.\n",
    "\n",
    "These partials all equal 0 which gives: \n",
    "$$ \\delta_bL = \\Sigma_{i=1}^l(\\alpha_i^* -\\alpha_i) \\\\  \n",
    "\\delta_wL = w - \\Sigma_{i=1}^l x_i(\\alpha_i^ -\\alpha_i^*) \\\\\n",
    "\\delta_{\\xi_i^{(*)}}L = \\delta_{\\xi_i^*}L = C - \\alpha_i^{(*)} - \\eta_i^{(*)}$$\n",
    "\n",
    "These can then be substituted back into our Lagrange function:\n",
    "\n",
    "$$\\text{maximize} \\begin{cases}\n",
    "-\\frac{1}{2} \\Sigma_{i,j=1}^l (\\alpha_i-\\alpha_i^*)(\\alpha_j-\\alpha_j^*)<x_i.x_j>\\\\\n",
    "-\\epsilon\\Sigma_{i=1}^l(\\alpha_i+\\alpha_i^*) + \\Sigma_{i=1}^ly_i(\\alpha_i-\\alpha_i^*)\\end{cases} \\\\\n",
    "\\text{subject to} \\Sigma_{i=1}^l(\\alpha_i-\\alpha_i^*) = 0 \\text{and} \\alpha_i, \\alpha_i^* \\in [0,C]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-soccer",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "(10 points) Repeat Q3 in HW1 using support vector regression. You can use any package this time (e.g., scikit-learn's SVR implementation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "transparent-validation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svr', SVR(epsilon=0.2))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_price(tick,start='2020-10-01',end=None):\n",
    "    return yf.Ticker(tick).history(start=start,end=end)['Close']\n",
    "\n",
    "def get_prices(tickers,start='2020-10-01',end=None):\n",
    "    df=pd.DataFrame()\n",
    "    for s in tickers:\n",
    "        df[s]=get_price(s,start,end)\n",
    "    return df\n",
    "\n",
    "feature_stocks=['tsla','fb','twtr','amzn','nflx','gbtc','gdx','intc','dal','c']\n",
    "predict_stock='msft'\n",
    "\n",
    "# training set\n",
    "start_date_train='2020-10-01'\n",
    "end_date_train='2020-12-31'\n",
    "\n",
    "X_train=get_prices(feature_stocks,start=start_date_train,end=end_date_train)\n",
    "y_train=get_prices([predict_stock],start=start_date_train,end=end_date_train)\n",
    "\n",
    "# testing set\n",
    "start_date_test='2021-01-01' # end date omit, default is doday\n",
    "X_test=get_prices(feature_stocks,start=start_date_test)\n",
    "y_test=get_prices([predict_stock],start=start_date_test)\n",
    "\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "dummyFeatureX_train = np.array(pd.get_dummies(X_train[0]))\n",
    "dummyFeatureX_test = np.array(pd.get_dummies(X_test[0]))\n",
    "ytrain = np.ravel(y_train,)\n",
    "regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "regr.fit(X_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "express-elite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([218.17559645, 217.8798197 , 216.21813407, 216.10230375,\n",
       "       216.0850678 , 215.6176551 , 215.22703363, 215.15906639,\n",
       "       214.94520899, 214.96452617, 215.01883469, 215.21239336,\n",
       "       215.05585853, 215.69975801, 215.79971263, 215.80932964,\n",
       "       215.91675675, 216.10278161, 215.92729545, 216.08805395,\n",
       "       215.64840417, 215.85225246, 215.4678803 , 215.61189305,\n",
       "       215.23525863, 215.17263976, 215.05341804, 214.96288961,\n",
       "       214.91363886, 214.89795085, 214.89971195, 214.90094963,\n",
       "       214.89123284, 214.90827755, 214.91035836, 214.895755  ,\n",
       "       214.89319209, 214.89097688, 214.88853461, 214.89588524,\n",
       "       214.89512588, 214.91083549, 214.9057116 , 214.89268958,\n",
       "       214.89866211, 214.90086077, 214.89643728, 214.8967951 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictionsPrice = regr.predict(X_test) # uses model to make predictions\n",
    "y_predictionsPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-indie",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
